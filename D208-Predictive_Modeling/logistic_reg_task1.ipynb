{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D208 Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.feature_selection import f_regression, SelectKBest, RFE\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Research Question\n",
    "\n",
    "## A1. Research Question\n",
    "**Summarize one research question that is relevant to a real-world organizational situation captured in the data set you have selected and that you will answer using multiple linear regression in the initial model.**\n",
    "\n",
    "My research question is: what are contributing factors related to a patient's length of stay? At a higher level, what variables directly influence the 'initial days' a patient stays in a hospital?\n",
    "\n",
    "## B2. Goals of the Data Analysis. \n",
    "**Note: Ensure that your goals are within the scope of your research question and are represented in the available data.**\n",
    "\n",
    "The general goal of the data analysis is to discover which factors influence a patient's length of stay. By uncovering which variables are correlated, whether it be patient demographics, hospital procedures or similar details, medical hospitals can utilize these findings in multiple different way such as improving patient care through better practices. Examples of improving patient care include identifying the higher risk patients (those likely to have longer stays) to possibly come up with more targeted interventions and preventative care measures. As a result of the previously mentioned benefit, hospital resources may be more optimally allocated, resulting in quick patient turnover and reduced overall medical costs for both patients and medical centers.\n",
    "\n",
    "\n",
    "The purpose of data analysis for linear regression modeling is to prepare the data, uncover relationships between variables, and ensure it meets regression assumptions. This involves cleaning the data, exploring distributions and patterns, transforming variables as needed, and selecting relevant features. My goal is to identify trends and correlations between independent variables, which will aid in modeling preparation and potentially reduce dimensionality. Organizing features by data type will help manage the project and determine necessary data transformations for accurate modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Method Justification\n",
    "\n",
    "B.  Describe multiple linear regression methods by doing the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1.  Summarize four assumptions of a multiple linear regression model.\n",
    "Multiple linear regression relies on four main assumptions. These four assumptions are **linearity, independence, homoscedasticity, and normality**.\n",
    "\n",
    "Linearity is defined as an existing linear relationship between the dependent and independent variables. Any change applied to an independent variable will effect the dependent variable linearly. A simpler way to explain this is an existing \"straight line relationship between two variables\" (Investopedia Linear Relationship Definition, Formula, and Examples).\n",
    "\n",
    "Independence is defined as the residuals being independent. Residuals are also known as the error, which is the difference between a predicted value and actual value $(y-\\hat{y})$. Independence among residuals implies there is no correlation between residuals. In summary, the residuals are ***normally distributed***.\n",
    "\n",
    "Furthermore on the subject of residuals, Homoscedasticity is defined as the variance of residuals remaining constant across all independent values. A good visual would be visualizing a scatterplot with the regression line and plotted residuals. In the case of homoscedascticity, the residuals should have similar variance from their respective predicted values. In contrast, heterosceascticity would not have a constant variance among the residuals, resulting in patterns such as cone-shapes, u-shapes, etc.. (Homoscedasticity - Statistics Solutions)\n",
    "\n",
    "Normality -- ADD HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2.  Describe two benefits of using Python or R in support of various phases of the analysis.\n",
    "\n",
    "Python is my choice of programming language due to it's ease of use and robust use in data science. Python has many versatile libraries including numpy (numerical python operations), pandas (used for manipulation of dataframes), scipy (used for statistical operations), and matplotlib and seaborn (strong versatile visualization packages).\n",
    "\n",
    "During the wrangling phase, Python's pandas library proves to be incredibly useful for the manipulation of dataframes, including cleaning, transforming and creating new features. Numpy is also an analyst favorite with its' powerful performance with numerical operations on arrays, which is synonymical to dataframes.\n",
    "\n",
    "Data exploration and statistics phases also greatly benefits from the various functions within Python. The visual libraries including matplotlib and seaborn has versatile graphs that provide easy access to quick insights. Distribution charts, scatterplots and heatmaps are a few of my most used functions that help me assess the distribution and relative relation to features within the dataset. This results in decreased need for computing power as well as reduction in redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3.  Explain why multiple linear regression is an appropriate technique to use for analyzing the research question summarized in part I.\n",
    "\n",
    "This technique is an algorithm used for modeling the relationship between independent feature variables and the predictor variable. In this case, the research question dependent variable is length of stay. Multiple linear regression is an appropriate algorithm for use in predicting a variable considered to be (or used as) continuous.\n",
    "\n",
    "\n",
    "This is suitable for the research question because it not only allows for simultaneous research of variables that influence a patient's length of stay, (e.g. a combination of patient characteristics, health conditions and lifestyle) it allows for a deep dive into what feature variables may be associated to a patient's stay in a hospital. By quantifying the relationships between patient demographics, hospital stay information etc. this technique offers valuable insights into the range of correlation the independent and predictor variable may have. \n",
    "\n",
    "\n",
    "Furthermore, the benefits of this technique allows one to see the direct interaction between each variable and how the direct influence each other. An example could be diabetes is much more pronounced in patients in certain cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Data Cleaning/Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **C1.** Describe data cleaning goals/steps.\n",
    "\n",
    "`Describe your data cleaning goals and the steps used to clean the data to achieve the goals that align with your research question including your annotated code.`\n",
    "\n",
    "In line with my previous data cleaning approach from D206, my plan is to systematically assess and clean the dataframe as needed. First, I will evaluate the data types of each column and convert them to appropriate types such as integer, float, or boolean. Next, I will identify and address any duplicates or missing values by either removing, separating, or imputing them. For this dataset, duplicates and null values are not present. Finally, I will conduct a thorough review of the data to ensure its accuracy, which may involve correcting spelling errors, fixing typos, and standardizing text formatting for readability.\n",
    "\n",
    "To clean the dataset, I will begin by ensuring all zip codes are at least five digits by padding with zeros where necessary, addressing data loss issues from previous integer formatting. The timezone column will be standardized to align with the nine main time zones and converted to a categorical data type. Columns such as `readmission`, `soft_drink`, and other various health related variables with `yes/no` or `(1,0)` responses will be converted to boolean data types. Values in columns like `vitd_levels`, `total_charge`, and `additional_charges` will be rounded to two decimal places. Columns such as `population`, `children`, and `income` will be corrected from float to integer types.\n",
    "\n",
    "Columns including `marital`, `gender`, and other service-related fields will be designated as categorical data types. As columns like `income` fall within reasonable bounds, no further imputation will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in dataframe\n",
    "df = pd.read_csv('medical_raw_df.csv', index_col=[0])\n",
    "# lowercase columns\n",
    "df.columns = map(str.lower, df.columns)\n",
    "\n",
    "# verify no nulls\n",
    "assert df.isnull().sum().sum() == 0\n",
    "# verify no duplicates\n",
    "assert df.duplicated().sum() == 0\n",
    "\n",
    "    \n",
    "# change timezone column entries before changing data type\n",
    "tz_dict = {\n",
    "    \"America/Puerto_Rico\" : \"US - Puerto Rico\",\n",
    "    \"America/New_York\": \"US - Eastern\",\n",
    "    \"America/Detroit\" : \"US - Eastern\",\n",
    "    \"America/Indiana/Indianapolis\" : \"US - Eastern\",\n",
    "    \"America/Indiana/Vevay\" : \"US - Eastern\",\n",
    "    \"America/Indiana/Vincennes\" : \"US - Eastern\",\n",
    "    \"America/Kentucky/Louisville\" : \"US - Eastern\",\n",
    "    \"America/Toronto\" : \"US - Eastern\",\n",
    "    \"America/Indiana/Marengo\" : \"US - Eastern\",\n",
    "    \"America/Indiana/Winamac\" : \"US - Eastern\",\n",
    "    \"America/Chicago\" : \"US - Central\", \n",
    "    \"America/Menominee\" : \"US - Central\",\n",
    "    \"America/Indiana/Knox\" : \"US - Central\",\n",
    "    \"America/Indiana/Tell_City\" : \"US - Central\",\n",
    "    \"America/North_Dakota/Beulah\" : \"US - Central\",\n",
    "    \"America/North_Dakota/New_Salem\" : \"US - Central\",\n",
    "    \"America/Denver\" : \"US - Mountain\",\n",
    "    \"America/Boise\" : \"US - Mountain\",\n",
    "    \"America/Phoenix\" : \"US - Arizona\",\n",
    "    \"America/Los_Angeles\" : \"US - Pacific\",\n",
    "    \"America/Nome\" : \"US - Alaskan\",\n",
    "    \"America/Anchorage\" : \"US - Alaskan\",\n",
    "    \"America/Sitka\" : \"US - Alaskan\",\n",
    "    \"America/Yakutat\" : \"US - Alaskan\",\n",
    "    \"America/Adak\" : \"US - Aleutian\",\n",
    "    \"Pacific/Honolulu\" : 'US - Hawaiian'\n",
    "    }\n",
    "df.timezone.replace(tz_dict, inplace=True)\n",
    "\n",
    "# convert zip column to str, then fill 0s in entries\n",
    "df.zip = df.zip.astype('str').str.zfill(5)\n",
    "\n",
    "# changing datatypes\n",
    "# change columns to boolean data type\n",
    "to_bool = ['readmis',\n",
    "           'soft_drink',\n",
    "           'highblood',\n",
    "           'stroke',\n",
    "           'overweight',\n",
    "           'arthritis',\n",
    "           'diabetes',\n",
    "           'hyperlipidemia',\n",
    "           'backpain',\n",
    "           'anxiety',\n",
    "           'allergic_rhinitis',\n",
    "           'reflux_esophagitis',\n",
    "           'asthma']\n",
    "\n",
    "for col in to_bool:\n",
    "    df[col] = df[col].replace({'Yes':1, 'No':0}).astype(bool)\n",
    "\n",
    "\n",
    "# round entries in columns to only have two decimal places\n",
    "round_num = ['vitd_levels',\n",
    "             'totalcharge',\n",
    "             'additional_charges']\n",
    "for col in round_num:\n",
    "    df[col] = round(df[col], 2)\n",
    "\n",
    "# change columns to integer data type\n",
    "to_int = ['population',\n",
    "          'children',\n",
    "          'age',\n",
    "          'income',\n",
    "          'initial_days']\n",
    "for col in to_int:\n",
    "    df[col] = df[col].astype('int32')\n",
    "\n",
    "# change columns to categorical data type\n",
    "to_cat = ['marital',\n",
    "          'gender',\n",
    "          'initial_admin',\n",
    "          'services',\n",
    "          'item1',\n",
    "          'item2',\n",
    "          'item3',\n",
    "          'item4',\n",
    "          'item5', \n",
    "          'item6',\n",
    "          'item7',\n",
    "          'item8',\n",
    "          'timezone',\n",
    "          'state',\n",
    "          'complication_risk']\n",
    "for col in to_cat:\n",
    "    df[col] = df[col].astype('category')\n",
    "      \n",
    "# make columns more readable  \n",
    "columns = {'caseorder':'case_order',\n",
    "          'uid':'unique_id',\n",
    "          'readmis':'readmission',\n",
    "          'vitd_supp':'vitd_supplement',\n",
    "          'highblood':'high_blood',\n",
    "          'services':'services_received',\n",
    "          'totalcharge':'total_charges',\n",
    "          'initial_days':'hospital_stay_days'}\n",
    "\n",
    "\n",
    "df[['item1','item2', 'item3', 'item4']] = df[['item1','item2', 'item3', 'item4']].astype('int32')\n",
    "df[['item5','item6', 'item7', 'item8']] = df[['item5','item6', 'item7', 'item8']].astype('int32')\n",
    "\n",
    "\n",
    "df.rename(columns=columns, inplace=True)\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CaseOrder</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>interaction</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>zip</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>population</th>\n",
       "      <th>area</th>\n",
       "      <th>timezone</th>\n",
       "      <th>job</th>\n",
       "      <th>children</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>marital</th>\n",
       "      <th>gender</th>\n",
       "      <th>readmission</th>\n",
       "      <th>vitd_levels</th>\n",
       "      <th>doc_visits</th>\n",
       "      <th>full_meals_eaten</th>\n",
       "      <th>vitd_supplement</th>\n",
       "      <th>soft_drink</th>\n",
       "      <th>initial_admin</th>\n",
       "      <th>high_blood</th>\n",
       "      <th>stroke</th>\n",
       "      <th>complication_risk</th>\n",
       "      <th>overweight</th>\n",
       "      <th>arthritis</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>hyperlipidemia</th>\n",
       "      <th>backpain</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>allergic_rhinitis</th>\n",
       "      <th>reflux_esophagitis</th>\n",
       "      <th>asthma</th>\n",
       "      <th>services_received</th>\n",
       "      <th>hospital_stay_days</th>\n",
       "      <th>total_charges</th>\n",
       "      <th>additional_charges</th>\n",
       "      <th>item1</th>\n",
       "      <th>item2</th>\n",
       "      <th>item3</th>\n",
       "      <th>item4</th>\n",
       "      <th>item5</th>\n",
       "      <th>item6</th>\n",
       "      <th>item7</th>\n",
       "      <th>item8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>C412403</td>\n",
       "      <td>8cd49b13-f45a-4b47-a2bd-173ffa932c2f</td>\n",
       "      <td>3a83ddb66e2ae73798bdf1d705dc0932</td>\n",
       "      <td>Eva</td>\n",
       "      <td>AL</td>\n",
       "      <td>Morgan</td>\n",
       "      <td>35621</td>\n",
       "      <td>34.34960</td>\n",
       "      <td>-86.72508</td>\n",
       "      <td>2951</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>US - Central</td>\n",
       "      <td>Psychologist, sport and exercise</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>86575</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Male</td>\n",
       "      <td>False</td>\n",
       "      <td>19.14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Emergency Admission</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Medium</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Blood Work</td>\n",
       "      <td>10</td>\n",
       "      <td>3726.70</td>\n",
       "      <td>17939.40</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Z919181</td>\n",
       "      <td>d2450b70-0337-4406-bdbb-bc1037f1734c</td>\n",
       "      <td>176354c5eef714957d486009feabf195</td>\n",
       "      <td>Marianna</td>\n",
       "      <td>FL</td>\n",
       "      <td>Jackson</td>\n",
       "      <td>32446</td>\n",
       "      <td>30.84513</td>\n",
       "      <td>-85.22907</td>\n",
       "      <td>11303</td>\n",
       "      <td>Urban</td>\n",
       "      <td>US - Central</td>\n",
       "      <td>Community development worker</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>46805</td>\n",
       "      <td>Married</td>\n",
       "      <td>Female</td>\n",
       "      <td>False</td>\n",
       "      <td>18.94</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Emergency Admission</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>High</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Intravenous</td>\n",
       "      <td>15</td>\n",
       "      <td>4193.19</td>\n",
       "      <td>17613.00</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>F995323</td>\n",
       "      <td>a2057123-abf5-4a2c-abad-8ffe33512562</td>\n",
       "      <td>e19a0fa00aeda885b8a436757e889bc9</td>\n",
       "      <td>Sioux Falls</td>\n",
       "      <td>SD</td>\n",
       "      <td>Minnehaha</td>\n",
       "      <td>57110</td>\n",
       "      <td>43.54321</td>\n",
       "      <td>-96.63772</td>\n",
       "      <td>17125</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>US - Central</td>\n",
       "      <td>Chief Executive Officer</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>14370</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Female</td>\n",
       "      <td>False</td>\n",
       "      <td>18.06</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Elective Admission</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Medium</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Blood Work</td>\n",
       "      <td>4</td>\n",
       "      <td>2434.23</td>\n",
       "      <td>17505.19</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A879973</td>\n",
       "      <td>1dec528d-eb34-4079-adce-0d7a40e82205</td>\n",
       "      <td>cd17d7b6d152cb6f23957346d11c3f07</td>\n",
       "      <td>New Richland</td>\n",
       "      <td>MN</td>\n",
       "      <td>Waseca</td>\n",
       "      <td>56072</td>\n",
       "      <td>43.89744</td>\n",
       "      <td>-93.51479</td>\n",
       "      <td>2162</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>US - Central</td>\n",
       "      <td>Early years teacher</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>39741</td>\n",
       "      <td>Married</td>\n",
       "      <td>Male</td>\n",
       "      <td>False</td>\n",
       "      <td>16.58</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Elective Admission</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Medium</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Blood Work</td>\n",
       "      <td>1</td>\n",
       "      <td>2127.83</td>\n",
       "      <td>12993.44</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C544523</td>\n",
       "      <td>5885f56b-d6da-43a3-8760-83583af94266</td>\n",
       "      <td>d2f0425877b10ed6bb381f3e2579424a</td>\n",
       "      <td>West Point</td>\n",
       "      <td>VA</td>\n",
       "      <td>King William</td>\n",
       "      <td>23181</td>\n",
       "      <td>37.59894</td>\n",
       "      <td>-76.88958</td>\n",
       "      <td>5287</td>\n",
       "      <td>Rural</td>\n",
       "      <td>US - Eastern</td>\n",
       "      <td>Health promotion specialist</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1209</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Female</td>\n",
       "      <td>False</td>\n",
       "      <td>17.44</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>Elective Admission</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Low</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>CT Scan</td>\n",
       "      <td>1</td>\n",
       "      <td>2113.07</td>\n",
       "      <td>3716.53</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CaseOrder customer_id                           interaction  \\\n",
       "0          1     C412403  8cd49b13-f45a-4b47-a2bd-173ffa932c2f   \n",
       "1          2     Z919181  d2450b70-0337-4406-bdbb-bc1037f1734c   \n",
       "2          3     F995323  a2057123-abf5-4a2c-abad-8ffe33512562   \n",
       "3          4     A879973  1dec528d-eb34-4079-adce-0d7a40e82205   \n",
       "4          5     C544523  5885f56b-d6da-43a3-8760-83583af94266   \n",
       "\n",
       "                          unique_id          city state        county    zip  \\\n",
       "0  3a83ddb66e2ae73798bdf1d705dc0932           Eva    AL        Morgan  35621   \n",
       "1  176354c5eef714957d486009feabf195      Marianna    FL       Jackson  32446   \n",
       "2  e19a0fa00aeda885b8a436757e889bc9   Sioux Falls    SD     Minnehaha  57110   \n",
       "3  cd17d7b6d152cb6f23957346d11c3f07  New Richland    MN        Waseca  56072   \n",
       "4  d2f0425877b10ed6bb381f3e2579424a    West Point    VA  King William  23181   \n",
       "\n",
       "        lat       lng  population      area      timezone  \\\n",
       "0  34.34960 -86.72508        2951  Suburban  US - Central   \n",
       "1  30.84513 -85.22907       11303     Urban  US - Central   \n",
       "2  43.54321 -96.63772       17125  Suburban  US - Central   \n",
       "3  43.89744 -93.51479        2162  Suburban  US - Central   \n",
       "4  37.59894 -76.88958        5287     Rural  US - Eastern   \n",
       "\n",
       "                                job  children  age  income   marital  gender  \\\n",
       "0  Psychologist, sport and exercise         1   53   86575  Divorced    Male   \n",
       "1      Community development worker         3   51   46805   Married  Female   \n",
       "2           Chief Executive Officer         3   53   14370   Widowed  Female   \n",
       "3               Early years teacher         0   78   39741   Married    Male   \n",
       "4       Health promotion specialist         1   22    1209   Widowed  Female   \n",
       "\n",
       "   readmission  vitd_levels  doc_visits  full_meals_eaten  vitd_supplement  \\\n",
       "0        False        19.14           6                 0                0   \n",
       "1        False        18.94           4                 2                1   \n",
       "2        False        18.06           4                 1                0   \n",
       "3        False        16.58           4                 1                0   \n",
       "4        False        17.44           5                 0                2   \n",
       "\n",
       "   soft_drink        initial_admin  high_blood  stroke complication_risk  \\\n",
       "0       False  Emergency Admission        True   False            Medium   \n",
       "1       False  Emergency Admission        True   False              High   \n",
       "2       False   Elective Admission        True   False            Medium   \n",
       "3       False   Elective Admission       False    True            Medium   \n",
       "4        True   Elective Admission       False   False               Low   \n",
       "\n",
       "   overweight  arthritis  diabetes  hyperlipidemia  backpain  anxiety  \\\n",
       "0       False       True      True           False      True     True   \n",
       "1        True      False     False           False     False    False   \n",
       "2        True      False      True           False     False    False   \n",
       "3       False       True     False           False     False    False   \n",
       "4       False      False     False            True     False    False   \n",
       "\n",
       "   allergic_rhinitis  reflux_esophagitis  asthma services_received  \\\n",
       "0               True               False    True        Blood Work   \n",
       "1              False                True   False       Intravenous   \n",
       "2              False               False   False        Blood Work   \n",
       "3              False                True    True        Blood Work   \n",
       "4               True               False   False           CT Scan   \n",
       "\n",
       "   hospital_stay_days  total_charges  additional_charges item1 item2 item3  \\\n",
       "0                  10        3726.70            17939.40     3     3     2   \n",
       "1                  15        4193.19            17613.00     3     4     3   \n",
       "2                   4        2434.23            17505.19     2     4     4   \n",
       "3                   1        2127.83            12993.44     3     5     5   \n",
       "4                   1        2113.07             3716.53     2     1     3   \n",
       "\n",
       "  item4 item5 item6 item7 item8  \n",
       "0     2     4     3     3     4  \n",
       "1     4     4     4     3     3  \n",
       "2     4     3     4     3     3  \n",
       "3     3     4     5     5     5  \n",
       "4     3     5     3     4     3  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaseOrder                int64\n",
       "customer_id             object\n",
       "interaction             object\n",
       "unique_id               object\n",
       "city                    object\n",
       "state                 category\n",
       "county                  object\n",
       "zip                     object\n",
       "lat                    float64\n",
       "lng                    float64\n",
       "population               int32\n",
       "area                    object\n",
       "timezone              category\n",
       "job                     object\n",
       "children                 int32\n",
       "age                      int32\n",
       "income                   int32\n",
       "marital               category\n",
       "gender                category\n",
       "readmission               bool\n",
       "vitd_levels            float64\n",
       "doc_visits               int64\n",
       "full_meals_eaten         int64\n",
       "vitd_supplement          int64\n",
       "soft_drink                bool\n",
       "initial_admin         category\n",
       "high_blood                bool\n",
       "stroke                    bool\n",
       "complication_risk     category\n",
       "overweight                bool\n",
       "arthritis                 bool\n",
       "diabetes                  bool\n",
       "hyperlipidemia            bool\n",
       "backpain                  bool\n",
       "anxiety                   bool\n",
       "allergic_rhinitis         bool\n",
       "reflux_esophagitis        bool\n",
       "asthma                    bool\n",
       "services_received     category\n",
       "hospital_stay_days       int32\n",
       "total_charges          float64\n",
       "additional_charges     float64\n",
       "item1                    int32\n",
       "item2                    int32\n",
       "item3                    int32\n",
       "item4                    int32\n",
       "item5                    int32\n",
       "item6                    int32\n",
       "item7                    int32\n",
       "item8                    int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify datatypes were changed accordingly\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2.  Summary Statistics for Independent and Dependent Variable.\n",
    "\n",
    "Describe the dependent variable and all independent variables using summary statistics that are required to answer the research question, including a screenshot of the summary statistics output for each of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate exploratory variables into type for ease of exploring\n",
    "\n",
    "# continuous variables\n",
    "cont_vars = ['age',\n",
    "             'income',\n",
    "             'vitd_levels',\n",
    "             'total_charges',\n",
    "             'additional_charges',\n",
    "             'population',\n",
    "             'children',\n",
    "             'doc_visits',\n",
    "             'full_meals_eaten',\n",
    "             'vitd_levels',\n",
    "             'vitd_supplement',\n",
    "             'item2',\n",
    "             'item3',\n",
    "             'item4',\n",
    "             'item5',\n",
    "             'item6',\n",
    "             'item7',\n",
    "             'item8']\n",
    "\n",
    "# categorical variables\n",
    "cat_vars = ['gender', \n",
    "            'marital',\n",
    "            'area',\n",
    "            'timezone', \n",
    "            'initial_admin',\n",
    "            'complication_risk',\n",
    "            'services_received']\n",
    "\n",
    "\n",
    "# List of boolean health-related variables\n",
    "boolean_vars = ['readmission',\n",
    "                'high_blood', \n",
    "                'stroke',  \n",
    "                'overweight', \n",
    "                'arthritis', \n",
    "                'diabetes', \n",
    "                'hyperlipidemia', \n",
    "                'backpain', \n",
    "                'anxiety', \n",
    "                'allergic_rhinitis', \n",
    "                'reflux_esophagitis', \n",
    "                'asthma',\n",
    "                'full_meals_eaten',\n",
    "                'soft_drink']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Target: hospital_stay_days Stats***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for target variable `initial days`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    10000.00\n",
       "mean        33.96\n",
       "std         26.30\n",
       "min          1.00\n",
       "25%          7.00\n",
       "50%         35.50\n",
       "75%         61.00\n",
       "max         71.00\n",
       "Name: hospital_stay_days, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Summary statistics for target variable `hospital_stay_days`')\n",
    "round(df.hospital_stay_days.describe(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Continuous Variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for continuous feature columns:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>53.511700</td>\n",
       "      <td>20.638538</td>\n",
       "      <td>18.00</td>\n",
       "      <td>36.0000</td>\n",
       "      <td>53.00</td>\n",
       "      <td>71.00</td>\n",
       "      <td>89.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>40490.002100</td>\n",
       "      <td>28521.152883</td>\n",
       "      <td>154.00</td>\n",
       "      <td>19598.2500</td>\n",
       "      <td>33768.00</td>\n",
       "      <td>54295.75</td>\n",
       "      <td>207249.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vitd_levels</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>17.964272</td>\n",
       "      <td>2.017259</td>\n",
       "      <td>9.81</td>\n",
       "      <td>16.6275</td>\n",
       "      <td>17.95</td>\n",
       "      <td>19.35</td>\n",
       "      <td>26.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_charges</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>5312.172758</td>\n",
       "      <td>2180.393815</td>\n",
       "      <td>1938.31</td>\n",
       "      <td>3179.3750</td>\n",
       "      <td>5213.95</td>\n",
       "      <td>7459.70</td>\n",
       "      <td>9180.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>additional_charges</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>12934.528592</td>\n",
       "      <td>6542.601554</td>\n",
       "      <td>3125.70</td>\n",
       "      <td>7986.4850</td>\n",
       "      <td>11573.98</td>\n",
       "      <td>15626.49</td>\n",
       "      <td>30566.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>population</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>9965.253800</td>\n",
       "      <td>14824.758614</td>\n",
       "      <td>0.00</td>\n",
       "      <td>694.7500</td>\n",
       "      <td>2769.00</td>\n",
       "      <td>13945.00</td>\n",
       "      <td>122814.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>children</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>2.097200</td>\n",
       "      <td>2.163659</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_visits</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>5.012200</td>\n",
       "      <td>1.045734</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full_meals_eaten</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>1.001400</td>\n",
       "      <td>1.008117</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vitd_levels</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>17.964272</td>\n",
       "      <td>2.017259</td>\n",
       "      <td>9.81</td>\n",
       "      <td>16.6275</td>\n",
       "      <td>17.95</td>\n",
       "      <td>19.35</td>\n",
       "      <td>26.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vitd_supplement</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.398900</td>\n",
       "      <td>0.628505</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item2</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>3.506700</td>\n",
       "      <td>1.034825</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item3</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>3.511100</td>\n",
       "      <td>1.032755</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item4</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>3.515100</td>\n",
       "      <td>1.036282</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item5</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>3.496900</td>\n",
       "      <td>1.030192</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item6</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>3.522500</td>\n",
       "      <td>1.032376</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item7</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>3.494000</td>\n",
       "      <td>1.021405</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item8</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>3.509700</td>\n",
       "      <td>1.042312</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count          mean           std      min         25%  \\\n",
       "age                 10000.0     53.511700     20.638538    18.00     36.0000   \n",
       "income              10000.0  40490.002100  28521.152883   154.00  19598.2500   \n",
       "vitd_levels         10000.0     17.964272      2.017259     9.81     16.6275   \n",
       "total_charges       10000.0   5312.172758   2180.393815  1938.31   3179.3750   \n",
       "additional_charges  10000.0  12934.528592   6542.601554  3125.70   7986.4850   \n",
       "population          10000.0   9965.253800  14824.758614     0.00    694.7500   \n",
       "children            10000.0      2.097200      2.163659     0.00      0.0000   \n",
       "doc_visits          10000.0      5.012200      1.045734     1.00      4.0000   \n",
       "full_meals_eaten    10000.0      1.001400      1.008117     0.00      0.0000   \n",
       "vitd_levels         10000.0     17.964272      2.017259     9.81     16.6275   \n",
       "vitd_supplement     10000.0      0.398900      0.628505     0.00      0.0000   \n",
       "item2               10000.0      3.506700      1.034825     1.00      3.0000   \n",
       "item3               10000.0      3.511100      1.032755     1.00      3.0000   \n",
       "item4               10000.0      3.515100      1.036282     1.00      3.0000   \n",
       "item5               10000.0      3.496900      1.030192     1.00      3.0000   \n",
       "item6               10000.0      3.522500      1.032376     1.00      3.0000   \n",
       "item7               10000.0      3.494000      1.021405     1.00      3.0000   \n",
       "item8               10000.0      3.509700      1.042312     1.00      3.0000   \n",
       "\n",
       "                         50%       75%        max  \n",
       "age                    53.00     71.00      89.00  \n",
       "income              33768.00  54295.75  207249.00  \n",
       "vitd_levels            17.95     19.35      26.39  \n",
       "total_charges        5213.95   7459.70    9180.73  \n",
       "additional_charges  11573.98  15626.49   30566.07  \n",
       "population           2769.00  13945.00  122814.00  \n",
       "children                1.00      3.00      10.00  \n",
       "doc_visits              5.00      6.00       9.00  \n",
       "full_meals_eaten        1.00      2.00       7.00  \n",
       "vitd_levels            17.95     19.35      26.39  \n",
       "vitd_supplement         0.00      1.00       5.00  \n",
       "item2                   3.00      4.00       7.00  \n",
       "item3                   4.00      4.00       8.00  \n",
       "item4                   4.00      4.00       7.00  \n",
       "item5                   3.00      4.00       7.00  \n",
       "item6                   4.00      4.00       7.00  \n",
       "item7                   3.00      4.00       7.00  \n",
       "item8                   3.00      4.00       7.00  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# statistics for continuous variables\n",
    "continuous_summ = df[cont_vars].describe()\n",
    "print('Summary statistics for continuous feature columns:\\n')\n",
    "continuous_summ.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Categorical Variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution for categorical variables:\n",
      "\n",
      "gender:\n",
      "Female       50.18\n",
      "Male         47.68\n",
      "Nonbinary     2.14\n",
      "Name: gender, dtype: float64\n",
      "\n",
      "marital:\n",
      "Widowed          20.45\n",
      "Married          20.23\n",
      "Separated        19.87\n",
      "Never Married    19.84\n",
      "Divorced         19.61\n",
      "Name: marital, dtype: float64\n",
      "\n",
      "area:\n",
      "Rural       33.69\n",
      "Suburban    33.28\n",
      "Urban       33.03\n",
      "Name: area, dtype: float64\n",
      "\n",
      "timezone:\n",
      "US - Eastern        43.26\n",
      "US - Central        37.92\n",
      "US - Pacific         9.37\n",
      "US - Mountain        6.98\n",
      "US - Arizona         1.00\n",
      "US - Alaskan         0.69\n",
      "US - Puerto Rico     0.43\n",
      "US - Hawaiian        0.34\n",
      "US - Aleutian        0.01\n",
      "Name: timezone, dtype: float64\n",
      "\n",
      "initial_admin:\n",
      "Emergency Admission      50.60\n",
      "Elective Admission       25.04\n",
      "Observation Admission    24.36\n",
      "Name: initial_admin, dtype: float64\n",
      "\n",
      "complication_risk:\n",
      "Medium    45.17\n",
      "High      33.58\n",
      "Low       21.25\n",
      "Name: complication_risk, dtype: float64\n",
      "\n",
      "services_received:\n",
      "Blood Work     52.65\n",
      "Intravenous    31.30\n",
      "CT Scan        12.25\n",
      "MRI             3.80\n",
      "Name: services_received, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# distribution of categorical variables\n",
    "print(\"\\nDistribution for categorical variables:\")\n",
    "for var in cat_vars:\n",
    "    print(f'\\n{var}:')\n",
    "    print(df[var].value_counts(normalize=True) * 100)  # display percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency distribution for boolean variables:\n",
      "\n",
      "readmission:\n",
      "False    63.31\n",
      "True     36.69\n",
      "Name: readmission, dtype: float64\n",
      "\n",
      "high_blood:\n",
      "False    59.1\n",
      "True     40.9\n",
      "Name: high_blood, dtype: float64\n",
      "\n",
      "stroke:\n",
      "False    80.07\n",
      "True     19.93\n",
      "Name: stroke, dtype: float64\n",
      "\n",
      "overweight:\n",
      "True     70.94\n",
      "False    29.06\n",
      "Name: overweight, dtype: float64\n",
      "\n",
      "arthritis:\n",
      "False    64.26\n",
      "True     35.74\n",
      "Name: arthritis, dtype: float64\n",
      "\n",
      "diabetes:\n",
      "False    72.62\n",
      "True     27.38\n",
      "Name: diabetes, dtype: float64\n",
      "\n",
      "hyperlipidemia:\n",
      "False    66.28\n",
      "True     33.72\n",
      "Name: hyperlipidemia, dtype: float64\n",
      "\n",
      "backpain:\n",
      "False    58.86\n",
      "True     41.14\n",
      "Name: backpain, dtype: float64\n",
      "\n",
      "anxiety:\n",
      "False    67.85\n",
      "True     32.15\n",
      "Name: anxiety, dtype: float64\n",
      "\n",
      "allergic_rhinitis:\n",
      "False    60.59\n",
      "True     39.41\n",
      "Name: allergic_rhinitis, dtype: float64\n",
      "\n",
      "reflux_esophagitis:\n",
      "False    58.65\n",
      "True     41.35\n",
      "Name: reflux_esophagitis, dtype: float64\n",
      "\n",
      "asthma:\n",
      "False    71.07\n",
      "True     28.93\n",
      "Name: asthma, dtype: float64\n",
      "\n",
      "full_meals_eaten:\n",
      "0    37.15\n",
      "1    36.15\n",
      "2    18.56\n",
      "3     6.12\n",
      "4     1.69\n",
      "5     0.25\n",
      "6     0.06\n",
      "7     0.02\n",
      "Name: full_meals_eaten, dtype: float64\n",
      "\n",
      "soft_drink:\n",
      "False    74.25\n",
      "True     25.75\n",
      "Name: soft_drink, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Frequency distribution for boolean variables\n",
    "print('Frequency distribution for boolean variables:')\n",
    "for col in boolean_vars:\n",
    "    print(f'\\n{col}:')\n",
    "    print(df[col].value_counts(normalize=True) * 100)  # display percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C3.  Univariate & Bivariate Visualizations\n",
    "- distributions of the dependent and independent variables, including the dependent variable in your bivariate visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Target Variable***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## target column\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['hospital_stay_days'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['hospital_stay_days'])\n",
    "    \n",
    "plt.grid(linewidth=0.3)\n",
    "plt.suptitle('Distribution of Hospital Length of Stay')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Continuous Variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution plot along with boxplot\n",
    "for col in cont_vars:\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # hist\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(data=df, x=col)\n",
    "    \n",
    "    plt.xlabel(col)\n",
    "    \n",
    "    # boxplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(df[col])\n",
    "    plt.xlabel(col)\n",
    "     \n",
    "    plt.grid(linewidth=0.3)    \n",
    "    plt.suptitle(f'Distribution of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Categorical Variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of categorical variables\n",
    "print('Distribution for categorical variables:')\n",
    "for var in cat_vars:\n",
    "    df[var].value_counts().plot(kind='bar') \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f'Distribution of {var}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Indicator (boolean) Variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency distribution for boolean variables\n",
    "print('Frequency distribution for boolean variables:')\n",
    "for col in boolean_vars:\n",
    "    plt.figure(figsize=(12,6))\n",
    "#     print(f'\\n{col}:')\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[col].value_counts().plot(kind='bar')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    val_counts_dict = dict(df[col].value_counts())\n",
    "    \n",
    "    v = list(val_counts_dict.values())\n",
    "    k = list(val_counts_dict.keys())\n",
    "    plt.pie(v, labels = k, shadow=True)\n",
    "    plt.axis('equal')\n",
    "    plt.suptitle(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bivariate Visualizations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Continuous v. Continuous (target)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cont_vars:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.regplot(data=df, x=df['hospital_stay_days'], y=i)\n",
    "    plt.title(f'Correlation of initial days to {i}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Continuous to Categorical Variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cat_vars:\n",
    "    print(f'{i} to hospital_stay_days')\n",
    "    sns.boxplot(data=df, x=i, y='hospital_stay_days')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.grid(linewidth=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in boolean_vars:\n",
    "    plt.figure(figsize=(5,7))\n",
    "    sns.boxplot(x=i, y='hospital_stay_days', data=df, notch=True)\n",
    "    plt.title(f'{i} to hospital_stay_days')\n",
    "    plt.grid(linewidth=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.  Describe your data transformation goals that align with your research question and the steps used to transform the data to achieve the goals, including the annotated code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prepare the data for modeling there a few required steps. Algorithms are not built to read text so nominal categorical variables would be engineered into dummy columns. Ordinal variables can often remain the same since they tend to be ordered numbers where the higher number can equate to a higher coefficient. In this dataset specifically we do not have any applicable ordinal variables and all present discrete numeric values are nominal. In this scenario we also must prepare our nominal variables as numerical values. So columns with Yes No values will be changed to 1 0 as needed. From here, the last step for this portion would be creating dummy columns for each discrete variable that does not need order preserved (such as ordinal columns).\n",
    "\n",
    "Numerical values, specifically continuous values should also be scaled for the model to score properly. Every numerical feature will have a different minimum and maximum, thus it is important to place them on a similar scale. For this dataframe specifically, I prefer to use `sklearn` minmax scaler which sets a column's minimum to 0 and maximum to 1. I will also only employ this on the continuous variables.\n",
    "\n",
    "Unique identifiers such as customer_id, interaction and unique_id are removed as they have no relevance to the prediction of stay. I also chose to remove job and geographical columns like zip,  during this round. Normally I would approach this nominal category by doing heavy exploration and testing binning certains jobs into groups, zipcodes in hemispheres etc.. As of now the juice is not necessarily worth the squeeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take a look at dataframe info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing INCOME for ethics purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep For Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns will be excluded from our initial model. Identifier fields such as case_order, customer_id, interaction, and unique_id are not suitable for use as features in the modeling as they serve purely to identify observations and thus hold no predictive value.\n",
    "\n",
    "Other columns such as those related to location (e.g., city, state, zip) and variables like income and job, will also be omitted in this iteration for ethical considerations and practical analysis constraints. The job feature in particular, contains a large variety of values, making it less efficient to create dummy variables for each category. Instead, methods like clustering or binning could be explored in the future to group jobs meaningfully. Given that this is a minimum viable product (MVP) model, such advanced pre-processing steps can be deferred until necessary.\n",
    "\n",
    "\n",
    "DISCRETE VALUES normally values have a small range (e.g., 0 to 5), they can often be used directly in the model without scaling, as they do not typically cause issues with the model's performance or interpretation.\n",
    "\n",
    "Scaling (if necessary): For discrete values with a broader range or if used alongside continuous variables with large ranges, scaling might be applied to ensure balanced contributions and avoid skewing the model.\n",
    "\n",
    "Categorical Encoding: If discrete values represent categories (e.g., number of rooms, types of buildings), they may be treated as continuous or converted into dummy variables (binary indicators) if the categories are nominal (i.e., unordered). This transformation allows the model to interpret these features appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove unnecessary columns not used for modeling\n",
    "remove_cols = ['CaseOrder',\n",
    "               'customer_id',\n",
    "               'interaction',\n",
    "               'unique_id',\n",
    "               'city',\n",
    "               'state',\n",
    "               'county',\n",
    "               'zip',\n",
    "               'lat',\n",
    "               'lng',\n",
    "               'income',\n",
    "               'job']\n",
    "\n",
    "df.drop(columns = remove_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nominal Categorical\n",
    "\n",
    "The columns listed above are nominal categorical variables, meaning they represent categories without any inherent order or ranking. For use in a linear regression model, these categorical variables need to be transformed into numerical representations. However, assigning arbitrary numerical values to each category (e.g., 1 for female and 2 for male) would incorrectly imply a meaningful order, which could mislead the model. To address this, we use dummy variables, which convert each category into a binary (0 or 1) representation, ensuring that no ordinal relationship is inferred between the categories. This process enables the model to correctly interpret the categorical data without the issue of bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummify nominal\n",
    "to_dummy = ['gender'\n",
    "           ,'marital'\n",
    "           ,'initial_admin'\n",
    "           ,'services_received'\n",
    "           ,'complication_risk'\n",
    "           ,'area'\n",
    "           ,'timezone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies for the specified columns\n",
    "dummy_df = pd.get_dummies(df[to_dummy], drop_first=True)\n",
    "\n",
    "# Concatenate the original dataframe with the dummies\n",
    "df = pd.concat([df, dummy_df], axis=1)\n",
    "\n",
    "# # drop the original columns \n",
    "df.drop(columns = to_dummy, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[to_dummy].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling Numerical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below list `to_scale` is the list of features I will scale. Scaling (also known as standardizing) numerical values is necessary for the model particularly because linear regression models estimate coefficients for the independent variables. These coefficients represent the effect of each feature on the dependent variable. When numerical values (particularly continuous) with a heirarchy ranking order have a large enough range, it is necessary to scale them so the models do not mdisproportionately assign coefficients that cause an imbalance in the scale. As a result, this helps the model treat all continuous equally.\n",
    "\n",
    "There are several methods for scaling data, each with different applications. The `MinMaxScaler()` from `sklearn` scales numerical values to a range between 0 and 1. Another favorite method of mine is the `RobustScaler()` which is particularly useful for datasets with outliers. It sets the median to 0 and scales the data using the interquartile range (IQR), making it more robust to outliers. Since this dataset does not contain many outliers, I will opt for the `MinMaxScaler()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale continuous values (discrete on a case by case depending on range)\n",
    "to_scale = ['age'\n",
    "            ,'vitd_levels'\n",
    "            ,'total_charges'\n",
    "            ,'additional_charges'\n",
    "            ,'population'\n",
    "            ,'children'\n",
    "            ,'doc_visits'\n",
    "            ,'full_meals_eaten'\n",
    "            ,'vitd_supplement'\n",
    "            ,'item1'\n",
    "            ,'item2'\n",
    "            ,'item3'\n",
    "            ,'item4'\n",
    "            ,'item5'\n",
    "            ,'item6'\n",
    "            ,'item7'\n",
    "            ,'item8']\n",
    "\n",
    "print(df[to_scale].dtypes)\n",
    "\n",
    "df[to_scale].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['item1','item2', 'item3', 'item4']] = df[['item1','item2', 'item3', 'item4']].astype('int32')\n",
    "df[['item5','item6', 'item7', 'item8']] = df[['item5','item6', 'item7', 'item8']].astype('int32')\n",
    "print(df[to_scale].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df[to_scale] = scaler.fit_transform(df[to_scale])\n",
    "\n",
    "df[to_scale].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boolean Value Prep\n",
    "\n",
    "In linear regression, boolean values are handled by converting them to binary numerical values (1, 0). This conversion allows the model to use these columns as features. For a boolean feature, such as whether a person is ***overweight***, the coefficient in the regression model represents the effect of that feature being `True` compared to `False`. Specifically if the boolean is True, the model adds the value of the coefficient to the prediction. However, if False, it does not affect the prediction (e.g. $weight 0 x = 0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change boolean to binary\n",
    "binary = ['readmission'\n",
    "                  ,'high_blood'\n",
    "                  ,'stroke'\n",
    "                  ,'overweight'\n",
    "                  ,'arthritis'\n",
    "                  ,'diabetes'\n",
    "                  ,'hyperlipidemia'\n",
    "                  ,'backpain'\n",
    "                  ,'anxiety'\n",
    "                  ,'allergic_rhinitis'\n",
    "                  ,'reflux_esophagitis'\n",
    "                  ,'asthma']\n",
    "\n",
    "\n",
    "# replace True with 1's and False with 0's\n",
    "df[binary] = df[binary].replace(True, 1)\n",
    "df[binary] = df[binary].replace(False, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[binary].head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C5.  Provide the prepared data set as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV file here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle/Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate, test = train_test_split(df,\n",
    "                                        test_size=.2, \n",
    "                                        random_state=314,\n",
    "                                        shuffle=True)\n",
    "\n",
    "train, validate = train_test_split(train_validate,\n",
    "                                   test_size=.3, \n",
    "                                   random_state=314,\n",
    "                                   shuffle=True)\n",
    "\n",
    "\n",
    "print(f'Train shape ---> {train.shape}')\n",
    "print(f'Validate shape ---> {validate.shape}')\n",
    "print(f'Test shape ---> {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kitchen_sink_cols = ['area'\n",
    "#                     ,'children'\n",
    "#                     ,'age'\n",
    "#                     ,'marital'\n",
    "#                     ,'gender'\n",
    "#                     ,'readmission'\n",
    "#                     ,'vitd_levels'\n",
    "#                     ,'doc_visits'\n",
    "#                     ,'full_meals_eaten'\n",
    "#                     ,'vitd_supplement'\n",
    "#                     ,'soft_drink'\n",
    "#                     ,'initial_admin'\n",
    "#                     ,'high_blood'\n",
    "#                     ,'stroke'\n",
    "#                     ,'complication_risk'\n",
    "#                     ,'overweight'\n",
    "#                     ,'arthritis'\n",
    "#                     ,'diabetes'\n",
    "#                     ,'hyperlipidemia'\n",
    "#                     ,'backpain'\n",
    "#                     ,'anxiety'\n",
    "#                     ,'allergic_rhinitis'\n",
    "#                     ,'reflux_esophagitis'\n",
    "#                     ,'asthma'\n",
    "#                     ,'services_received'\n",
    "#                     ,'total_charges'\n",
    "#                     ,'additional_charges'\n",
    "#                     ,'item1'\n",
    "#                     ,'item2'\n",
    "#                     ,'item3'\n",
    "#                     ,'item4'\n",
    "#                     ,'item5'\n",
    "#                     ,'item6'\n",
    "#                     ,'item7'\n",
    "#                     ,'item8']\n",
    "\n",
    "\n",
    "X_train = train.drop(columns='hospital_stay_days')\n",
    "X_validate = validate.drop(columns='hospital_stay_days')\n",
    "X_test = test.drop(columns='hospital_stay_days')\n",
    "\n",
    "y_train = train.hospital_stay_days\n",
    "y_validate = validate.hospital_stay_days\n",
    "y_test = test.hospital_stay_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "y_validate= pd.DataFrame(y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kitchen Sink: Initial Baseline Model\n",
    "- choose cols for training\n",
    "- scale accordingly\n",
    "- create dummies\n",
    "- shuffle then split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = LinearRegression(normalize=True)\n",
    "target = 'hospital_stay_days'\n",
    "model_name = 'linear_reg'\n",
    "\n",
    "# fit the model using the algorithm\n",
    "algo.fit(X_train, y_train[target])\n",
    "\n",
    "# predict train\n",
    "y_train[model_name] = algo.predict(X_train)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train[target], y_train[model_name])**(1/2)\n",
    "\n",
    "# predict validate\n",
    "y_validate[model_name] = algo.predict(X_validate)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate[target], y_validate[model_name])**(1/2)\n",
    "\n",
    "print(\"RMSE for\", model_name, \"using\", algo, \"\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Model Comparison and Analysis\n",
    "\n",
    "D.  Compare an initial and a reduced linear regression model by doing the following:\n",
    "\n",
    "1.  Construct an initial multiple linear regression model from all independent variables that were identified in part C2.\n",
    "\n",
    "2.  Justify a statistically based feature selection procedure or a model evaluation metric to reduce the initial model in a way that aligns with the research question.\n",
    "\n",
    "3.  Provide a reduced linear regression model that follows the feature selection or model evaluation process in part D2, including a screenshot of the output for each model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "E.  Analyze the data set using your reduced linear regression model by doing the following:\n",
    "\n",
    "1.  Explain your data analysis process by comparing the initial multiple linear regression model and reduced linear regression model, including the following element:\n",
    "\n",
    "•   a model evaluation metric\n",
    "\n",
    "2.  Provide the output and all calculations of the analysis you performed, including the following elements for your reduced linear regression model:\n",
    "\n",
    "•   a residual plot\n",
    "\n",
    "•   the model’s residual standard error\n",
    "\n",
    "3.  Provide an executable error-free copy of the code used to support the implementation of the linear regression models using a Python or R file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V: Data Summary and Implications\n",
    "\n",
    "F.  Summarize your findings and assumptions by doing the following:\n",
    "\n",
    "1.  Discuss the results of your data analysis, including the following elements:\n",
    "\n",
    "•   a regression equation for the reduced model\n",
    "\n",
    "•   an interpretation of the coefficients of the reduced model\n",
    "\n",
    "•   the statistical and practical significance of the reduced model\n",
    "\n",
    "•   the limitations of the data analysis\n",
    "\n",
    "2.  Recommend a course of action based on your results.\n",
    "\n",
    "\n",
    "\n",
    "Part VI: Demonstration\n",
    "\n",
    "G.  Provide a Panopto video recording that includes the presenter and a vocalized demonstration of the functionality of the code used for the analysis of the programming environment, including the following elements:\n",
    "\n",
    "•   an identification of the version of the programming environment\n",
    "\n",
    "•   a comparison of the initial multiple linear regression model you used and the reduced linear regression model you used in your analysis\n",
    "\n",
    "•   an interpretation of the coefficients of the reduced model\n",
    "\n",
    "\n",
    "Note: The audiovisual recording should feature you visibly presenting the material (i.e., not in voiceover or embedded video) and should simultaneously capture both you and your multimedia presentation.\n",
    "\n",
    "\n",
    "Note: For instructions on how to access and use Panopto, use the \"Panopto How-To Videos\" web link provided below. To access Panopto's website, navigate to the web link titled \"Panopto Access,\" and then choose to log in using the “WGU” option. If prompted, log in using your WGU student portal credentials, and then it will forward you to Panopto’s website.\n",
    "\n",
    "\n",
    "To submit your recording, upload it to the Panopto drop box titled “Regression Modeling – NBM3 | D208.” Once the recording has been uploaded and processed in Panopto's system, retrieve the URL of the recording from Panopto and copy and paste it into the Links option. Upload the remaining task requirements using the Attachments option.\n",
    "\n",
    "\n",
    "\n",
    "H.  List the web sources used to acquire data or segments of third-party code to support the application. Ensure the web sources are reliable.\n",
    "\n",
    "\n",
    "I.  Acknowledge sources, using in-text citations and references, for content that is quoted, paraphrased, or summarized.\n",
    "\n",
    "\n",
    "J.  Demonstrate professional communication in the content and presentation of your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "[Investopedia - Linear Relationship Definition](https://www.investopedia.com/terms/l/linearrelationship.asp)\n",
    "\n",
    "[Residual Analysis for Independence (helpful image for visualization)](https://dev.to/ungest/independence-of-errors-a-guide-to-validating-linear-regression-assumptions-4h6b)\n",
    "\n",
    "[Homoscedasticity - Statistics SolutionsPart IV: Model Comparison and Analysis\n",
    "\n",
    "D.  Compare an initial and a reduced linear regression model by doing the following:\n",
    "\n",
    "1.  Construct an initial multiple linear regression model from all independent variables that were identified in part C2.\n",
    "\n",
    "2.  Justify a statistically based feature selection procedure or a model evaluation metric to reduce the initial model in a way that aligns with the research question.\n",
    "\n",
    "3.  Provide a reduced linear regression model that follows the feature selection or model evaluation process in part D2, including a screenshot of the output for each model.\n",
    "](https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/homoscedasticity/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
