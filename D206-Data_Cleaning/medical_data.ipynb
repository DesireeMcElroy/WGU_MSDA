{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D206 Data Cleaning Performance Assessment\n",
    "\n",
    "***Desiree McElroy***\n",
    "- For this assessment, I have chosen the medical dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# A. Research Question\n",
    "Is there a correlation between specific geographical areas and the frequency of hospital readmissions? By examining various area characteristics such as location, city, state, and zip code, this research aims to identify patterns that may indicate higher susceptibility to repeated hospital visits. Understanding these correlations could offer valuable insights into potential factors influencing readmission rates and guide targeted interventions to improve patient outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd # for dataframe manipulation\n",
    "import numpy as np # to help with numerical operations in python\n",
    "\n",
    "import matplotlib.pyplot as plt # for visualizations\n",
    "import seaborn as sns # for visualizations\n",
    "\n",
    "import scipy.stats as stats # for statistics of columns\n",
    "from sklearn.decomposition import PCA # for principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the csv file and create dataframe\n",
    "df = pd.read_csv('medical_raw_data.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Variable Descriptions <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "\n",
    "\n",
    "***Describe all variables***\n",
    "\n",
    "Name | Description | Type | Example\n",
    ":---: | :---: | :---: | :--:\n",
    "CaseOrder | Case order number, also serves as an index | qualitative | 10000\n",
    "Customer_id | Unique ID given to each patient | qualitative | I569847\n",
    "Interaction | Unique ID given to interaction with patient | qualitative | bc482c02-f8c9-4423-99de-3db5e62a18d5\n",
    "uid | Unique ID given to interaction with patient | qualitative | 95663a202338000abdf7e09311c2a8a1\n",
    "city | patient's city per their billing statement | qualitative | Coraopolis\n",
    "state | patient's state per their billing statement | qualitative | PA\n",
    "county | patient's county of residence per their billing statement | qualitative | Allegheny\n",
    "zip | patient's residence zip code per their billing statement | qualitative | 15108\n",
    "lat | latitude coordinate of patient residence per billing statement | qualitative | 40.49998\n",
    "lng | longitude coordinate of patient residence per billing statement | qualitative | -80.19959\n",
    "population | population within a mile radius of patient | quantitative | 41524\n",
    "area | rural, urban or suburban area type | qualitative | Urban\n",
    "timezone | timezone of patient's residence | qualitative | America/New_York\n",
    "job | job of patient (or primary insurance holder) per their admissions information | qualitative | Sports development officer\n",
    "children | number of children in patient's household | quantitative | 8.0\n",
    "age | age of patient | quantitative | 53\n",
    "education | highest earned degree of patient | qualitative | 9th Grade to 12th Grade, No Diploma\n",
    "employment | ..... | qualitative | full time\n",
    "income | annual income of patient or primary insurance holder | quantitative | 62682.63\n",
    "marital | marital status of patient or primary insurance holder | qualitative | Separated\n",
    "gender | the customer's self identification of gender (male, female or nonbinary) | qualitative | Female\n",
    "readmis | whether patient was readmitted within a month of last release (yes or no) | qualitative | Yes\n",
    "vitd_levels | the patient's vitamin d level measured in ng/mL | quantitative | 20.421883\n",
    "doc_visits | number of times primary physician visited the patient during the initial hospitalization | quantitative | 5\n",
    "full_meals_eaten | number of full meals a patient ate while hospitalized, where partial meals are rounded to 0 | quantitative | 0\n",
    "vitd_supp | number of times vit d supplement was given to patient | quantitative | 1\n",
    "soft_drink | whether patient drinks three or more sodas a day (yes,no) | qualitative | no\n",
    "initial_admin | how patient was initially admitted into the hospital (emergency admission, elective admission, observation | qualitative | Observation Admission\n",
    "highblood | whether patient has high blood pressure (yes,no) | qualitative | No\n",
    "stroke | whether patient has had a stroke (yes,no) | qualitative | No\n",
    "complication_risk | level of complication risk assessed by physician (high, medium, low) | qualitative | Low\n",
    "overweight | whether patient is considered overweight based on age, gender and height (yes, no) | qualitative | 1.0\n",
    "arthritis | whether patient has arthritis (yes, no) | qualitative | Yes\n",
    "diabetes | whether patient has diabetes (yes, no) | qualitative | No\n",
    "hyperlipidemia | whether patient has hyperlipidemia (yes, no) | qualitative | Yes\n",
    "backpain | whether patient has chronic backpain (yes, no) | qualitative | No\n",
    "anxiety | whether patient has anxiety (yes, no) | qualitative | 0.0\n",
    "allergic_rhinitis | whether patient has allergic rhinitis (yes, no) | qualitative | Yes\n",
    "reflux_esophagitis | whether patient has reflux esophagitis (yes, no) | qualitative | No\n",
    "asthma | whether patient has asthma (yes, no) | qualitative | No\n",
    "services | primary service patient has received while hospitalized (blood work, intravenous, CT scan, MRI) | qualitative | Blood Work\n",
    "initial_days | number of days patient stayed in the hospital during initial visit | quantitative | 70.850592\n",
    "totalcharge | The amount charged to the patient daily. This value reflects an average per patient based on the total charge divided by the number of days hospitalized This amount reflects the typical charges billed to patient not including specialized treatments | quantitative | 8700.856021\n",
    "additional_charges | average amount charge to patient for misc procedures, treatments, medications, anesthesiology | quantitative | 11643.18993\n",
    "Item1 | Timely admission | qualitative | 4\n",
    "Item2 | Timely treatment | qualitative | 3\n",
    "Item3 | represents timely visits | qualitative | 3\n",
    "Item4 | represents reliability | qualitative | 2\n",
    "Item5 | represents options | qualitative | 3\n",
    "Item6 | represents hours of treatment | quantitative | 6\n",
    "Item7 | represents courteous staff | qualitative | 4\n",
    "Item8 | represents evidence of active listening from doctor | qualitative | 3\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C1.  Data Cleaning Plan\n",
    "\n",
    "**Propose a plan that includes the relevant techniques and specific steps needed to assess the quality of the data in the data set.**\n",
    "\n",
    "\n",
    "a. Get dataframe information summary using info function.\n",
    "\n",
    "b. Get an idea of the column data types using dtypes function in order to assess whether any data types need to be changed and how to visualize them.\n",
    "\n",
    "c. Visualize the distribution for continuous data, get an idea of value counts, or run .describe() on column. This will help me assess the type of distribution if any and how to address any empty and anamolous values. This will also help assess whether certain data points are truly anomalous.\n",
    "\n",
    "c. Analyze the null values, look for any commonality between nulls and assess how to handle them whether it be via imputation, dropping them, or separating them from the dataframe.\n",
    "\n",
    "d. View the outliers as needed, assess how to handle them, decide whether they are in reasonable bounds or not.\n",
    "\n",
    "e. Address any duplicates if present.\n",
    "\n",
    "f. Conduct principal component analysis to uncover any excessive variables and explore relationships.\n",
    "\n",
    "g. Lastly, address any duplicates if any."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2: Justify Data Cleaning Plan\n",
    "- This is thorough scope of analyzing the data. I start by getting a quick scope of the data by usinf the .info() function which allows me see the names of all columns, their data types, how many missing values are in each function as well as the dataframe shape.\n",
    "- Next, going column by column allows me to assess the data individually. For example with continuous value columns, I am able to assess the distribution and value counts where necessary. With discrete data, I am able to get a general view of frequency count.\n",
    "- Analyzing the nulls in each column with missing values allows me to assess any patterns in the missing data and decide how to handle the nulls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C3: Justify Programming Languages/Packages\n",
    "- **Python** is a powerful data science programming language that not only provides ease of use but is also extremely robust in implementing the entire data science pipeline on various types if data including tabular, text and even images.\n",
    "- Since our data is all maintained in a dataframe, **pandas** and **numpy** are the obvious first choice libraries to assess and clean this data. Numpy is a powerful python library that has many manipulation techniques particularly for arrays such as this dataframe. Pandas is a powerful library specifically for tabular data just like this dataframe. It allows to exploration and manipulation of columns, rows, data types, mathematical analysis etc...\n",
    "- Visuals also provide a powerful analysis of tabular data and thus **matplotlib** and **seaborn** are the top two choices for visuals. Each provide strong visualization graphs to help easily assess data quality, distribution and general data relationships. The most helpful functions for our particularly data are bar charts, distribution charts, and box plots.\n",
    "- For PCA, we will use the PCA functions from the **sklearn** library. These are needed to ultimately obtain the eigenvalues and implement with a scree plot to correctly identify linear relationships between some continuous columns from the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C4: Code to assess data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info on column names, data types, count and nulls.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('My dataframe has ', df.shape[0], 'rows and ', df.shape[1], 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all column names for ease of use\n",
    "df.columns = map(str.lower, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caseorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at caseorder column, verify unique values, there should be 10,000\n",
    "df.caseorder.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at customer_id, verify unique count of values, should be 10,000\n",
    "df.customer_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify interaction has 10,000 unique count of values\n",
    "df.interaction.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify uid has unique count of values, 10,000\n",
    "df.uid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess value counts of city column\n",
    "df.city.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# assess value counts of state column, verify suitable for categorical\n",
    "print('Data type is:', df.state.dtypes)\n",
    "print('Amount of unique entries:', df.state.value_counts().count())\n",
    "df.state.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess value counts of county column\n",
    "# variable is suitable as categorical\n",
    "print(df.county.dtypes)\n",
    "print('------')\n",
    "df.county.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some zip codes are missing numbers\n",
    "df.zip.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# assess zip codes missing numbers\n",
    "for i in range(1000):\n",
    "    if len(str(df.zip.tolist()[i]))<5:\n",
    "        print(df.zip.tolist()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess value counts of area column, we can see each is represented fairly\n",
    "print(df.area.dtypes)\n",
    "print(df.area.value_counts())\n",
    "print('------------')\n",
    "df.area.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view list of different time zones\n",
    "plt.figure(figsize=(10,10))\n",
    "df.timezone.value_counts().sort_values().plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather stats on data, datatype makes more sense as int\n",
    "print('Data type is:', df.children.dtypes)\n",
    "df.children.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data exists within reasonable bounds, data type should be int\n",
    "print(df.age.describe())\n",
    "print('--------')\n",
    "# data is uniformly distributed\n",
    "sns.histplot(data=df, x='age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data and type, suitable for categorical\n",
    "print(df.education.dtypes)\n",
    "df.education.value_counts().plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution and data typem suitable for categorical\n",
    "print(df.employment.dtypes)\n",
    "df.employment.value_counts().plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some data doesnt seem to exist within reasonable bounds, possible outliers\n",
    "df.income.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Dist of Income')\n",
    "sns.histplot(df.income)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing outliers in income\n",
    "plt.title('Boxplot of Income')\n",
    "sns.boxplot(data=df, x='income')\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at some obvious low income outliers, \n",
    "# could be from error when entered or these values may very well be possible\n",
    "df.income.nsmallest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this visualization and quick assessment, the lower end values could be assumed outliers but are still within two standard deviations of the mean and are not technically anomalous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is also uniformly distributed, suitable for categorical dtype\n",
    "print('Data type is:', df.marital.dtypes)\n",
    "df.marital.value_counts().plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype currently object but can be categorical\n",
    "print('Data type is:', df.gender.dtypes)\n",
    "df.gender.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readmis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype may be categorical or boolean\n",
    "df.readmis.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vit D Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize dist of vit d levels, assert dtype can remain float\n",
    "sns.histplot(data=df, x='vitd_levels')\n",
    "plt.title('Dist of Vit D Levels')\n",
    "plt.show()\n",
    "print('---------')\n",
    "\n",
    "# assess descripion of data\n",
    "print(df.vitd_levels.describe())\n",
    "print('---------')\n",
    "\n",
    "# assess outliers\n",
    "plt.title('Boxplot of Vit D Levels')\n",
    "sns.boxplot(data=df, x='vitd_levels')\n",
    "# plt.figure(figsize=(16,9))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We arguably have a right skewed or bimodal distribution. Despite most values around 30 or above are shown as outliers, they have a steady normal distribution and thus there is no reason to remove these values. Further investigation is needed before separating these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc_visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view data details, suitable as float or int\n",
    "df.doc_visits.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify suitable as int\n",
    "print('Data type is:', df.doc_visits.dtypes)\n",
    "df.doc_visits.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full_meals_eaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess distribution of meals eaten, verify suitable as int\n",
    "print('Data type is:', df.full_meals_eaten.dtypes)\n",
    "print('-----')\n",
    "print(df.full_meals_eaten.describe())\n",
    "print('-------')\n",
    "df.full_meals_eaten.value_counts().plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vitd_supp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess value counts of Vitd supp, verify suitable as int\n",
    "print('Data type is:', df.vitd_supp.dtypes)\n",
    "df.vitd_supp.value_counts().plot(kind='barh')\n",
    "plt.title('VitD Supp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soft_drink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suitable dtype as bool\n",
    "print('Data type is:', df.soft_drink.dtypes)\n",
    "df.soft_drink.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial_admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data suitable as categorical\n",
    "print('Data type is:', df.initial_admin.dtypes)\n",
    "df.initial_admin.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### highblood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data suitable as bool\n",
    "print('Data type is:', df.highblood.dtypes)\n",
    "df.highblood.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data suitable as bool\n",
    "print('Data type is:', df.stroke.dtypes)\n",
    "df.stroke.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### complication_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data suitable as categorical\n",
    "print('Data type is:', df.complication_risk.dtypes)\n",
    "df.complication_risk.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data suitable as bool\n",
    "print('Data type is:', df.overweight.dtypes)\n",
    "df.overweight.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arthritis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data suitable as bool\n",
    "print('Data type is:', df.arthritis.dtypes)\n",
    "df.arthritis.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data suitable as bool\n",
    "print('Data type is:', df.diabetes.dtypes)\n",
    "df.diabetes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperlipidemia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data suitable as bool\n",
    "print('Data type is:', df.hyperlipidemia.dtypes)\n",
    "df.hyperlipidemia.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backpain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data suitable as bool\n",
    "print('Data type is:', df.backpain.dtypes)\n",
    "df.backpain.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anxiety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data suitable as bool\n",
    "print('Data type is:', df.anxiety.dtypes)\n",
    "df.anxiety.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### allergic_rhinitis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data suitable as bool\n",
    "print('Data type is:', df.allergic_rhinitis.dtypes)\n",
    "df.allergic_rhinitis.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### asthma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data suitable as bool\n",
    "print('Data type is:', df.asthma.dtypes)\n",
    "df.asthma.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data suitable as categorical\n",
    "print('Data type is:', df.services.dtypes)\n",
    "df.services.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution, verify data suitable as int\n",
    "print('Data type is:', df.initial_days.dtypes)\n",
    "sns.histplot(data=df, x='initial_days')\n",
    "plt.title('Dist of Initial Days')\n",
    "plt.show()\n",
    "print('------')\n",
    "df.initial_days.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### totalcharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data suitable as float\n",
    "# amounts are within reasonable bounds\n",
    "print('Data type is:', df.totalcharge.dtypes)\n",
    "sns.histplot(data=df, x='totalcharge')\n",
    "plt.title('Dist of Total Charge')\n",
    "plt.show()\n",
    "print('------')\n",
    "#verify data is within reasonable bounds\n",
    "print(df.totalcharge.describe())\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional_charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data suitable as float\n",
    "# amounts are within reasonable bounds\n",
    "print('Data type is:', df.additional_charges.dtypes)\n",
    "sns.histplot(data=df, x='additional_charges')\n",
    "plt.title('Dist of Addtl Charges')\n",
    "plt.show()\n",
    "\n",
    "print(df.additional_charges.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data only consists of 1-8 options, dtype suitable for categorical\n",
    "print('Data type is:', df.item1.dtypes)\n",
    "df.item1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data only consists of 1-8 options, dtype suitable for categorical\n",
    "print('Data type is:', df.item2.dtypes)\n",
    "df.item2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data only consists of 1-8 options, dtype suitable for categorical\n",
    "print('Data type is:', df.item3.dtypes)\n",
    "df.item3.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data only consists of 1-8 options, dtype suitable for categorical\n",
    "print('Data type is:', df.item4.dtypes)\n",
    "df.item4.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data only consists of 1-8 options, dtype suitable for categorical\n",
    "print('Data type is:', df.item5.dtypes)\n",
    "df.item5.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data only consists of 1-8 options, dtype suitable for categorical\n",
    "print('Data type is:', df.item6.dtypes)\n",
    "df.item6.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data only consists of 1-8 options, dtype suitable for categorical\n",
    "print('Data type is:', df.item7.dtypes)\n",
    "df.item7.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data only consists of 1-8 options, dtype suitable for categorical\n",
    "print('Data type is:', df.item8.dtypes)\n",
    "df.item8.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "**Analyzing Nulls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a general print out of null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of columns with null values\n",
    "null_cols = df.columns[df.isna().any()].tolist()\n",
    "null_cols\n",
    "\n",
    "df[null_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in null_cols:\n",
    "    print('Total percentage of null values for', col, '--->', df[col].isna().sum()/len(df[col])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The children, age, income and soft_drink column have almost 25% of null values while overweight, anxiety and initial_days column are only missing roughly 10%. In any case, all columns have more than 5% missing values and thus we cannot simply drop them. The next step is to analyze the distributions where necessary and choose a valid imputation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze distribution of null columns\n",
    "for i in df[null_cols]:\n",
    "        if df[i].dtypes == 'object':\n",
    "            sns.catplot(data=df, x=i, kind='count')\n",
    "            \n",
    "        else:\n",
    "            sns.displot(df[i])\n",
    "            plt.xticks(fontsize= 12)\n",
    "            plt.yticks(fontsize=12)\n",
    "            plt.ylabel(\"Count\", fontsize= 13, fontweight=\"bold\")\n",
    "            plt.xlabel(i, fontsize=13, fontweight=\"bold\")\n",
    "            plt.title('Distribution of '+i)\n",
    "            plt.show()\n",
    "            print('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Null Column Takeaways:***\n",
    "\\\n",
    "Null columns type of distribution and imputation:\\\n",
    "    - **Children:** We are working with discrete data points and thus the best method of imputation for this column is using the mode.\\\n",
    "    - **Age:** Uniform Distribution, I want to go ahead impute with the mean since the data is symmetric\\\n",
    "    - **Income:** We have a skewed distribution and thus the best method for this would be using the median.\\\n",
    "    - **Overweight:** Working with categorical data it is best to use the mode.\\\n",
    "    - **Anxiety:** Again working with categorical data it is best to use the mode.\\\n",
    "    - **Initial_Days:** Here we have a bimodal distribution and thus the best recommendation is to use mode or median. Since our data points for this column are floats, I will use the median.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze duplicates, there are no duplicates\n",
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D1: Describe Data Quality Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zip**: This column had numerous entries that were not 5 digit zip codes. Analysis found some zip codes were missing 1 or more numbers. There is no way of knowing which numbers are missing. If we guess some 0s were stripped in the beginning since they were stored as integers, we would have to assume the zip codes with missing numbers lost the first or two digits because they were 0s and thus we can address this.\n",
    "\n",
    "**Population**: This column entered as a float and can be changed to int.\n",
    "\n",
    "**Timezone**: This column is not organized and condensed to the standard US time zones and thus I will change this to standard time zones. \n",
    "\n",
    "**Children**: This column is unnecessarily stored as a float and thus can be changed to int. This column also has 25% of values missing. The distribution is skewed right and there may be a pattern in higher ages not being entered.\n",
    "\n",
    "**Age**: This column is also unnecessarily stores as a float and thus will be changed to int. This column is also missing 25% of the data. Since it is a uniformed distribution it is best to impute missing values with the mean.\n",
    "\n",
    "**Income**: Again, this column is also unnecessarily stores as a float and thus will be changed to int. This column is also missing 25% of values and will be imputed using the median.\n",
    "\n",
    "**Marital**: This column is entered as string but should be changed to cateogrical.\n",
    "\n",
    "**Gender**: This column is entered as string but should be changed to cateogrical. According to the data dictionary, gender should have three options, female, male and nonbinary. In the dataframe the three options are female, male and prefer not to answer. Generally we cannot presume that \"prefer not to answer\" should be listed as nonbinary but for the sake of this assignment, I will assume the data dictionary is referencing that the prefer not to answer option should be nonbinary.\n",
    "\n",
    "\n",
    "**Readmis**: This column is stored as an object but should instead be boolean.\n",
    "\n",
    "**vitd_levels**: This column is appropriately stored as a float but  has an excessive amount of decimal points and thus it makes more sense to round to 2 decimal points.\n",
    "\n",
    "**soft_drink**: This column is stored as object but should be boolean.\n",
    "\n",
    "**initial_admin**: Stored as object but more appropriate as categorical.\n",
    "\n",
    "**highblood**: Stored as object but more appropriate as bool.\n",
    "\n",
    "**stroke**: Stored as object but more appropriate as bool.\n",
    "\n",
    "**complication_risk**: Stored as object but more appropriate as bool.\n",
    "\n",
    "**overweight**: Stored as float but more appropriate as bool. For missing values I will impute using the mode.\n",
    "\n",
    "**arthritis**: Stored as object but more appropriate as bool.\n",
    "\n",
    "**diabetes**: Stored as object but more appropriate as bool.\n",
    "\n",
    "**hyperlipidemia**: Stored as object but more appropriate as bool.\n",
    "\n",
    "**backpain**: Stored as object but more appropriate as bool.\n",
    "\n",
    "**anxiety**: Stored as float but more appropriate as bool. For missing values I will impute using the mode.\n",
    "\n",
    "**allergic_rhinitis**: Stored as object but more appropriate as bool.\n",
    "\n",
    "**reflux_esophagitis**: Stored as object but more appropriate as bool.\n",
    "\n",
    "**asthma**: Stored as object but more appropriate as bool.\n",
    "\n",
    "**services**: Stored as object but more appropriate as categorical.\n",
    "\n",
    "**initial_days**: Stored as float but is more appropriate as an int. For missing values I will impute using the median.\n",
    "\n",
    "**totalcharge**: Has an excessive amount of decimal points and thus I will change it to two decimal points.\n",
    "\n",
    "**additional_charges**: Also has an excessive amount of decimal points and thus I will change it to two decimal points.\n",
    "\n",
    "**item1**: Column is stored as int but needs to be changed to categorical.\n",
    "\n",
    "**item2**: Column is stored as int but needs to be changed to categorical.\n",
    "\n",
    "**item3**: Column is stored as int but needs to be changed to categorical.\n",
    "\n",
    "**item4**: Column is stored as int but needs to be changed to categorical.\n",
    "\n",
    "**item5**: Column is stored as int but needs to be changed to categorical.\n",
    "\n",
    "**item6**: Column is stored as int but needs to be changed to categorical.\n",
    "\n",
    "**item7**: Column is stored as int but needs to be changed to categorical.\n",
    "\n",
    "**item8**: Column is stored as int but needs to be changed to categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D2: Mitigating data quality issues\n",
    " - The zip column will be properly corrected so that all zip entries are uniformed to be five digits long. I will input a 0 (or 00 as needed) in the beginning of the zip code for any entry containing less than five digits. This will undo the loss of data that occurred when this column was set as an int which likely stripped the 0s from the column.\n",
    " - The timezone column will be standardized to the standard nine timezones. I will do this by separating the unique entries into a list using the variable as the correct timezone and loop through to replace the entry in the row. From there I will change the datatype to categorical.\n",
    " - The gender column wil be changed for prefer not to say entries to reflect nonbinary instead. This is only an assumption for this class. In general, more information would be needed. I will map the string prefer not to say and change it to nonbinary.\n",
    " - The columns readmis, soft_drink, highblood, stroke, complication_risk, overweight, arthritis, diabetes, hyperlipidema, backpain, anxiety, allergic_rhinitis, reflux_esophagitis and asthma are all columns that need to be changed to boolean since they only have two possible answers and all either use the standard 'yes' or 'no' or 1 or 0. A boolean datatype is the most appropriate for these columns. I will make a list, loop through and use the astype() function to change the datatype to boolean.\n",
    " - The columns vitd_levels, totalcharge and additional_charges will be rounded to two decimal places as six decimal places is too excessive. I will also add these to a list and loop through the dataframe applying the numpy .round() function.\n",
    " - The columns population, children, age, income and initial_days are all set as float currently but make more sense being an int data type. I will again add these names to a list, loop through my dataframe and apply the astype() function to change their datatype to int.\n",
    " - The columns marital, gender, initial_admin, services, item1, item2, item3, item4, item5, item6, item7 and item8 are all more suitable as a categorical datatype. I will also add these column names to a list that I will use to loop through the dataframe and use the astype() function to change the datatype to categorical.\n",
    " - No anomalous data could be confirmed without further investigation. Suspicious columns such as income are within reasonable bounds.\n",
    " - Columns with null values are children, age, income, overweight, anxiety and initial_days. The missing values per column are more than 5% and thus should not just be dropped. I will impute using the appropriate method for the data's distribution by first making missing values Nan then imputing using appropriate method (mean, median or mode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D3: Summarizing Data Cleaning Results\n",
    "The data cleaning methods will not only make the data easier to read and work with, but will enhance future analysis on this data. We are also ensuring data integrity but applying the correct data types to columns as needed. Columns with missing or incorrect data such as zip, will now be fully corrected and able to be analyzed with data integrity in mind. The dataframe also retained it's shape and no data was lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D4: Code used for Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in dataframe\n",
    "df = pd.read_csv('medical_raw_data.csv', index_col=[0])\n",
    "# lowercase columns\n",
    "df.columns = map(str.lower, df.columns)\n",
    "\n",
    "# fill nulls first so there are no issues when changing datatypes\n",
    "mode = ['children', 'overweight', 'anxiety']\n",
    "mean = ['age']\n",
    "median = ['income', 'initial_days']\n",
    "for col in mode:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "for col in mean:\n",
    "    df[col].fillna(df[col].mean(), inplace=True)\n",
    "for col in median:\n",
    "    df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "# change timezone column entries before changing data type\n",
    "tz_dict = {\n",
    "    \"America/Puerto_Rico\" : \"US - Puerto Rico\",\n",
    "    \"America/New_York\": \"US - Eastern\",\n",
    "    \"America/Detroit\" : \"US - Eastern\",\n",
    "    \"America/Indiana/Indianapolis\" : \"US - Eastern\",\n",
    "    \"America/Indiana/Vevay\" : \"US - Eastern\",\n",
    "    \"America/Indiana/Vincennes\" : \"US - Eastern\",\n",
    "    \"America/Kentucky/Louisville\" : \"US - Eastern\",\n",
    "    \"America/Toronto\" : \"US - Eastern\",\n",
    "    \"America/Indiana/Marengo\" : \"US - Eastern\",\n",
    "    \"America/Indiana/Winamac\" : \"US - Eastern\",\n",
    "    \"America/Chicago\" : \"US - Central\", \n",
    "    \"America/Menominee\" : \"US - Central\",\n",
    "    \"America/Indiana/Knox\" : \"US - Central\",\n",
    "    \"America/Indiana/Tell_City\" : \"US - Central\",\n",
    "    \"America/North_Dakota/Beulah\" : \"US - Central\",\n",
    "    \"America/North_Dakota/New_Salem\" : \"US - Central\",\n",
    "    \"America/Denver\" : \"US - Mountain\",\n",
    "    \"America/Boise\" : \"US - Mountain\",\n",
    "    \"America/Phoenix\" : \"US - Arizona\",\n",
    "    \"America/Los_Angeles\" : \"US - Pacific\",\n",
    "    \"America/Nome\" : \"US - Alaskan\",\n",
    "    \"America/Anchorage\" : \"US - Alaskan\",\n",
    "    \"America/Sitka\" : \"US - Alaskan\",\n",
    "    \"America/Yakutat\" : \"US - Alaskan\",\n",
    "    \"America/Adak\" : \"US - Aleutian\",\n",
    "    \"Pacific/Honolulu\" : 'US - Hawaiian'\n",
    "    }\n",
    "df.timezone.replace(tz_dict, inplace=True)\n",
    "\n",
    "# change prefer not to answer to nonbinary as per the data dictionary\n",
    "df.gender.replace({\n",
    "    'Prefer not to answer' : 'nonbinary'  \n",
    "}, inplace=True)\n",
    "\n",
    "# convert zip column to str, then fill 0s in entries\n",
    "df.zip = df.zip.astype('str').str.zfill(5)\n",
    "\n",
    "# changing datatypes\n",
    "# change columns to boolean data type\n",
    "to_bool = ['readmis', 'soft_drink', 'highblood', 'stroke',\n",
    "           'complication_risk', 'overweight', 'arthritis', 'diabetes',\n",
    "          'hyperlipidemia', 'backpain', 'anxiety', 'allergic_rhinitis',\n",
    "          'reflux_esophagitis', 'asthma']\n",
    "for col in to_bool:\n",
    "    df[col] = df[col].astype('bool')\n",
    "\n",
    "# round entries in columns to only have two decimal places\n",
    "round_num = ['vitd_levels', 'totalcharge', 'additional_charges']\n",
    "for col in round_num:\n",
    "    df[col] = round(df[col], 2)\n",
    "\n",
    "# change columns to integer data type\n",
    "to_int = ['population', 'children', 'age','income',\n",
    "         'initial_days']\n",
    "for col in to_int:\n",
    "    df[col] = df[col].astype('int32')\n",
    "\n",
    "# change columns to categorical data type\n",
    "to_cat = ['marital', 'gender', 'initial_admin', 'services',\n",
    "          'item1', 'item2', 'item3', 'item4', 'item5', \n",
    "          'item6', 'item7', 'item8', 'timezone', 'state',\n",
    "         'education', 'employment', 'complication_risk']\n",
    "for col in to_cat:\n",
    "    df[col] = df[col].astype('category')\n",
    "    \n",
    "    \n",
    "    \n",
    "# make columns more readable   \n",
    "columns = ['case_order', 'customer_id', 'interaction', 'unique_id', 'city', \n",
    "           'state', 'county', 'zip', 'latitude', 'longitude', 'population', 'area', \n",
    "           'timezone', 'job', 'children', 'age', 'education', 'employment', \n",
    "           'income', 'marital', 'gender', 'readmission', 'vitd_levels', 'doc_visits', \n",
    "           'full_meals_eaten', 'vitd_supplement', 'soft_drink', 'initial_admin', \n",
    "           'high_blood', 'stroke', 'complication_risk', 'overweight', 'arthritis', \n",
    "           'diabetes', 'hyperlipidemia', 'backpain', 'anxiety', 'allergic_rhinitis',\n",
    "           'reflux_esophagitis', 'asthma', 'services_received', 'initial_days', 'total_charge', \n",
    "           'additional_charges', 'item1', 'item2', 'item3', 'item4', 'item5', 'item6', \n",
    "           'item7', 'item8']\n",
    "\n",
    "df.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D5: CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_medical_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D6: Summarize Limitations of Cleaning Process\n",
    "\n",
    "Some data/domain knowledge was limited particularly with how `gender` should have been ideally handled. Another example was knowing what a normal vitamin D level should look like. A quick search makes our data look like most of the patients had severely low vitamin D. Things like this could be confirmed to maintain data integrity. Also further investigating into a possible cause of missing values may also provide more insight. Is there a pattern of missing values? For example, why are there almost no values for vitamin D levels between 30 and 40? This could all provide limitations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D7: How the Limitations Could Affect Analysis\n",
    "- Without having in depth knowledge of the domain and data, it is possible some changes were made incorrectly. For example, perhaps 'prefer not to say' was not meant to mean nonbinary. Without knowing the normal and abnormal range for vitamin D levels prevents us from conducting proper analysis. From a quick glance, most patients seem to have severely low vitamin D, but is there a possibly correlation between this and readmissions or is the data collected incorrectly? What is the explanation for the distribution not following the central limit theorem?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# E1.  Identify principal components, provide output of principal components loading matrix.\n",
    "\n",
    "**Principal Components** are quantifiable numeric and continous variables. The following meet the critera for further analysis.\n",
    "- population\n",
    "- income\n",
    "- vitd_levels\n",
    "- totalcharge\n",
    "- additional_charges\n",
    "- latitude\n",
    "- longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first assign continuous variables\n",
    "cont_var = ['population', 'income', 'vitd_levels', 'total_charge',\n",
    "            'additional_charges', 'latitude', 'longitude']\n",
    "\n",
    "cont_df = df[cont_var]\n",
    "cont_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the continuous df\n",
    "normalized_df = (cont_df - cont_df.mean())/cont_df.std()\n",
    "# get count of components\n",
    "n_components = cont_df.shape[1]\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(normalized_df)\n",
    "pca_df = pd.DataFrame(pca.transform(normalized_df),\n",
    "                     columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5',\n",
    "                               'PC6', 'PC7'])\n",
    "\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_columns = pca_df.columns\n",
    "loadings = pd.DataFrame(pca.components_.T,\n",
    "                       columns=pca_columns,\n",
    "                       index=cont_df.columns)\n",
    "loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relationship MIGHT exist but doesn't mean it is meaningful. Thus we implement the evaluation of eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = np.dot(normalized_df.T, normalized_df)/cont_df.shape[0]\n",
    "eigenvalues = [np.dot(eigenvector.T, np.dot(cov_matrix, eigenvector)) for eigenvector in pca.components_]\n",
    "\n",
    "plt.figure(figsize=(13,7))\n",
    "plt.plot(eigenvalues, linewidth=3)\n",
    "plt.axhline(y=1, color='r', linestyle='-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('eigenvalue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1\n",
    "for i in eigenvalues:\n",
    "    print('PCA',num, ' ', i)\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2.  Justify reduced number of the principal components\n",
    "\n",
    "We can see from above that PCA 1, 2 and 3 have eigenvalues above 1. For PCA 4, 5, 6, and 7, they fall below 1 and thus will be discarded.\n",
    "\n",
    "# E3. Describe how the organization would benefit from the use of PCA.\n",
    "Many things come to mind when referring to the benefits of using PCA. For one, it helps remove multicollinearty and thus reduce the dimensions which can be beneficial in reducing computational costs, or even with machine learning, it can helps in a situation where a model is overfit and thus improve performance. Specifically, the benefit is helping to \"find the most significant features\" in a dataset which can ultimately reduce redundant features or \"help find patterns in high-dimensional datasets (Simplilearn 2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G. Web Sources (Code)\n",
    "- https://seaborn.pydata.org/generated/seaborn.histplot.html\n",
    "- https://seaborn.pydata.org/generated/seaborn.boxplot.html\n",
    "\n",
    "# H. Resources\n",
    "- https://towardsdatascience.com/a-step-by-step-introduction-to-pca-c0d78e26a0dd\n",
    "- https://www.simplilearn.com/tutorials/machine-learning-tutorial/principal-component-analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
